### Port conventions (though there are many, many exceptions)
# 82xx - TBD
# 83xx - grapl plugin services, like grapl-aws-plugins
# 84xx - debugger ports (see vsc_debugger.py)

version: "3.8"
volumes:
  dgraph_export:
  pulumi_outputs:
    # This volume will be used to output Pulumi stack outputs that may
    # need to be accessible in test containers.

x-common-variables:
  aws-region: &aws-region
    AWS_DEFAULT_REGION: ${AWS_REGION} # boto3 prefers this one
    AWS_REGION: ${AWS_REGION}
  cloudwatch-env: &cloudwatch-env
    CLOUDWATCH_ACCESS_KEY_ID: ${CLOUDWATCH_ACCESS_KEY_ID}
    CLOUDWATCH_ACCESS_KEY_SECRET: ${CLOUDWATCH_ACCESS_KEY_SECRET}
    CLOUDWATCH_ENDPOINT: ${CLOUDWATCH_ENDPOINT}
  dgraph-env: &dgraph-env
    MG_ALPHAS: ${MG_ALPHAS}
  dynamodb-env: &dynamodb-env
    DYNAMODB_ACCESS_KEY_ID: ${DYNAMODB_ACCESS_KEY_ID}
    DYNAMODB_ACCESS_KEY_SECRET: ${DYNAMODB_ACCESS_KEY_SECRET}
    DYNAMODB_ENDPOINT: ${DYNAMODB_ENDPOINT}
  # TODO: Ensure these are matched with what's in provision_local_identity_table.py
  dynamodb-mapping-tables: &dynamodb-mapping-tables
    ASSET_ID_MAPPINGS: ${DEPLOYMENT_NAME}-asset_id_mappings
    DYNAMIC_SESSION_TABLE: ${DEPLOYMENT_NAME}-dynamic_session_table
    FILE_HISTORY_TABLE: ${DEPLOYMENT_NAME}-file_history_table
    INBOUND_CONNECTION_HISTORY_TABLE: ${DEPLOYMENT_NAME}-inbound_connection_history_table
    IP_CONNECTION_HISTORY_TABLE: ${DEPLOYMENT_NAME}-ip_connection_history_table
    NETWORK_CONNECTION_HISTORY_TABLE: ${DEPLOYMENT_NAME}-network_connection_history_table
    OUTBOUND_CONNECTION_HISTORY_TABLE: ${DEPLOYMENT_NAME}-outbound_connection_history_table
    PROCESS_HISTORY_TABLE: ${DEPLOYMENT_NAME}-process_history_table
    STATIC_MAPPING_TABLE: ${DEPLOYMENT_NAME}-static_mapping_table
  ec2-env: &ec2-env
    EC2_ACCESS_KEY_ID: ${EC2_ACCESS_KEY_ID}
    EC2_ACCESS_KEY_SECRET: ${EC2_ACCESS_KEY_SECRET}
    EC2_ENDPOINT: ${EC2_ENDPOINT}
  kafka-broker-env: &kafka-broker-env
    KAFKA_BROKER_ID: 1
    KAFKA_ZOOKEEPER_CONNECT: "${ZOOKEEPER_HOST}:${ZOOKEEPER_PORT}"
    KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
    KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://${KAFKA_BROKER_HOST}:9092,PLAINTEXT_HOST://localhost:29092
    KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
    KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
    KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
    KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
    KAFKA_JMX_PORT: ${KAFKA_JMX_PORT}
    KAFKA_JMX_HOSTNAME: localhost
    KAFKA_LOG4J_ROOT_LOGLEVEL: INFO
  lambda-env: &lambda-env
    LAMBDA_ACCESS_KEY_ID: ${LAMBDA_ACCESS_KEY_ID}
    LAMBDA_ACCESS_KEY_SECRET: ${LAMBDA_ACCESS_KEY_SECRET}
    LAMBDA_ENDPOINT: ${LAMBDA_ENDPOINT}
  log-level: &log-level
    GRAPL_LOG_LEVEL: "${GRAPL_LOG_LEVEL:-ERROR}"
    RUST_LOG: "${RUST_LOG:-ERROR}"
  route53-env: &route53-env
    ROUTE53_ACCESS_KEY_ID: ${ROUTE53_ACCESS_KEY_ID}
    ROUTE53_ACCESS_KEY_SECRET: ${ROUTE53_ACCESS_KEY_SECRET}
    ROUTE53_ENDPOINT: ${ROUTE53_ENDPOINT}
  rust-backtrace: &rust-backtrace
    RUST_BACKTRACE: ${RUST_BACKTRACE}
  s3-env: &s3-env
    S3_ACCESS_KEY_ID: ${S3_ACCESS_KEY_ID}
    S3_ACCESS_KEY_SECRET: ${S3_ACCESS_KEY_SECRET}
    S3_ENDPOINT: ${S3_ENDPOINT}
  secretsmanager-env: &secretsmanager-env
    SECRETSMANAGER_ACCESS_KEY_ID: ${SECRETSMANAGER_ACCESS_KEY_ID}
    SECRETSMANAGER_ACCESS_KEY_SECRET: ${SECRETSMANAGER_ACCESS_KEY_SECRET}
    SECRETSMANAGER_ENDPOINT: ${SECRETSMANAGER_ENDPOINT}
  sns-env: &sns-env
    SNS_ACCESS_KEY_ID: ${SNS_ACCESS_KEY_ID}
    SNS_ACCESS_KEY_SECRET: ${SNS_ACCESS_KEY_SECRET}
    SNS_ENDPOINT: ${SNS_ENDPOINT}
  sqs-env: &sqs-env
    SQS_ACCESS_KEY_ID: ${SQS_ACCESS_KEY_ID}
    SQS_ACCESS_KEY_SECRET: ${SQS_ACCESS_KEY_SECRET}
    SQS_ENDPOINT: ${SQS_ENDPOINT}
  ssm-env: &ssm-env
    SSM_ACCESS_KEY_ID: ${SSM_ACCESS_KEY_ID}
    SSM_ACCESS_KEY_SECRET: ${SSM_ACCESS_KEY_SECRET}
    SSM_ENDPOINT: ${SSM_ENDPOINT}
  zookeeper-env: &zookeeper-env
    ZOOKEEPER_CLIENT_PORT: ${ZOOKEEPER_PORT}
    ZOOKEEPER_TICK_TIME: 2000

services:
  ########################################################################
  # Cloud Infrastructure Dependencies
  ########################################################################

  dgraph:
    tty: false
    image: dgraph/standalone:v20.07.1
    ports:
      # required to access the RATEL interface for dgraph
      - 127.0.0.1:${DGRAPH_RATEL_HTTP_EXTERNAL_PUBLIC_PORT}:${DGRAPH_RATEL_HTTP_EXTERNAL_PUBLIC_PORT}
      # required for RATEL interface to operate properly
      - 127.0.0.1:${DGRAPH_ALPHA_HTTP_EXTERNAL_PUBLIC_PORT}:${DGRAPH_ALPHA_HTTP_EXTERNAL_PUBLIC_PORT}
    volumes:
      - type: volume
        source: dgraph_export
        # Hitting :8080/admin/export will force an export to be written to this directory.
        target: /dgraph/export
    networks:
      default:
        aliases:
          - ${DGRAPH_HOST}

  zookeeper:
    image: confluentinc/cp-zookeeper:6.2.0
    environment:
      <<: *zookeeper-env
    healthcheck:
      test: ["CMD", "echo", "ruok", "|", "nc", "-w", "2", "-q", "2", "localhost", "${ZOOKEEPER_PORT}", "|", "grep", "imok"]
      retries: 5
      interval: 5s
      timeout: 30s
      start_period: 15s

  kafka-broker:
    image: confluentinc/cp-kafka:6.2.0
    depends_on:
      zookeeper:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "nc", "-vz", "kafka-broker", "${KAFKA_BROKER_PORT}"]
      retries: 5
      interval: 5s
      timeout: 30s
      start_period: 15s
    environment:
      <<: *kafka-broker-env

  # dev uses 1 big redis instance, prod has 1:1 redis per grapl
  # service... maybe transitory, this will eventually match prod
  redis:
    image: redis:latest
    command: |
      sh -c "
        # hack from https://stackoverflow.com/questions/54533308/disable-redis-persistence-in-docker
        # to disable persistence
        rm -f /data/dump.rdb && redis-server
      "
    healthcheck:
      test: bash -c 'redis-cli -h 127.0.0.1 ping | grep PONG'
      interval: 5s
      timeout: 10s
      start_period: 10s
    networks:
      default:
        aliases:
          - ${REDIS_HOST}

  localstack:
    image: localstack/localstack:0.12.11
    ports:
      # We'll expose localstack's edge port for ease of use with
      # things like the AWS CLI, Pulumi, etc.
      - 127.0.0.1:${LOCALSTACK_PORT}:${LOCALSTACK_PORT}
    environment:
      - IMAGE_NAME=localstack/localstack:0.12.11
      - EDGE_PORT=${LOCALSTACK_PORT}
      - HOSTNAME_EXTERNAL=${LOCALSTACK_HOST}
      - SERVICES=apigateway,cloudwatch,dynamodb,ec2,events,iam,lambda,logs,s3,secretsmanager,sns,sqs
      - DEBUG=1
      # Once we put the lambdas behind the API gateway, overall test
      # time increased. Using the `docker` executor reliably takes at
      # least 2x the time of `docker-reuse`. However, the containers
      # generated are invisible to docker-compose, and Localstack
      # doesn't shut them down, so we have to manage that on our own
      # (see Makefile)
      - LAMBDA_EXECUTOR=docker-reuse
      - MAIN_CONTAINER_NAME=${COMPOSE_PROJECT_NAME}_localstack_1
      # Without this, the lambda containers are attached to the bridge network
      - LAMBDA_DOCKER_NETWORK=grapl-network
      - DATA_DIR=${DATA_DIR- }
    privileged: true # for docker lambda execution
    healthcheck:
      test: |
        bash -c '
          export AWS_ACCESS_KEY_ID=${FAKE_AWS_ACCESS_KEY_ID} &&
          export AWS_SECRET_ACCESS_KEY=${FAKE_AWS_SECRET_ACCESS_KEY} &&
          aws --endpoint-url=http://${LOCALSTACK_HOST}:${LOCALSTACK_PORT} s3 ls
        '
      # Probe failure during this period will not be counted towards the maximum number of retries
      start_period: 30s
      # Health check is executed every `interval` seconds.
      interval: 5s
      # If a single run of the check takes longer than `timeout` seconds then the check is considered to have failed.
      timeout: 10s
      # It takes `retries` consecutive failures of the health check for the container to be considered unhealthy.
      retries: 3
    volumes:
      - "/var/run/docker.sock:/var/run/docker.sock"
    networks:
      default:
        aliases:
          - ${LOCALSTACK_HOST}

  ########################################################################
  # Rust Services
  ########################################################################


  grapl-sysmon-subgraph-generator:
    image: grapl/grapl-sysmon-subgraph-generator:${TAG:-latest}
    tty: false
    environment:
      <<: *aws-region
      DEAD_LETTER_QUEUE_URL: "${SQS_ENDPOINT}/queue/${DEPLOYMENT_NAME}-sysmon-generator-dead-letter-queue"
      DEST_BUCKET_NAME: "${DEPLOYMENT_NAME}-unid-subgraphs-generated-bucket"
      REDIS_ENDPOINT: "${REDIS_ENDPOINT}"
      <<: *log-level
      RETRY_QUEUE_URL: "${SQS_ENDPOINT}/000000000000/${DEPLOYMENT_NAME}-sysmon-generator-retry-queue"
      <<: *s3-env
      SOURCE_QUEUE_URL: "${SQS_ENDPOINT}/000000000000/${DEPLOYMENT_NAME}-sysmon-generator-queue"
      <<: *sqs-env
      <<: *rust-backtrace
    depends_on:
      provisioner:
        condition: service_completed_successfully
      localstack:
        condition: service_healthy
      redis:
        condition: service_healthy

  grapl-osquery-subgraph-generator:
    image: grapl/grapl-osquery-subgraph-generator:${TAG:-latest}
    tty: false
    environment:
      <<: *aws-region
      DEPLOYMENT_NAME: ${DEPLOYMENT_NAME}
      DEAD_LETTER_QUEUE_URL: "${SQS_ENDPOINT}/queue/${DEPLOYMENT_NAME}-osquery-generator-dead-letter-queue"
      DEST_BUCKET_NAME: "${DEPLOYMENT_NAME}-unid-subgraphs-generated-bucket"
      REDIS_ENDPOINT: "${REDIS_ENDPOINT}"
      <<: *log-level
      RETRY_QUEUE_URL: "${SQS_ENDPOINT}/queue/${DEPLOYMENT_NAME}-osquery-generator-retry-queue"
      <<: *s3-env
      SOURCE_QUEUE_URL: "${SQS_ENDPOINT}/queue/${DEPLOYMENT_NAME}-osquery-generator-queue"
      <<: *sqs-env
      <<: *rust-backtrace
    depends_on:
      provisioner:
        condition: service_completed_successfully
      localstack:
        condition: service_healthy
      redis:
        condition: service_healthy

  grapl-node-identifier:
    image: grapl/grapl-node-identifier:${TAG:-latest}
    environment:
      <<: *aws-region
      DEAD_LETTER_QUEUE_URL: "${SQS_ENDPOINT}/queue/${DEPLOYMENT_NAME}-node-identifier-dead-letter-queue"
      DEST_BUCKET_NAME: "${DEPLOYMENT_NAME}-subgraphs-generated-bucket"
      <<: *dynamodb-env
      <<: *dynamodb-mapping-tables
      REDIS_ENDPOINT: "${REDIS_ENDPOINT}"
      <<: *log-level
      RETRY_QUEUE_URL: "${SQS_ENDPOINT}/queue/${DEPLOYMENT_NAME}-node-identifier-retry-queue"
      <<: *s3-env
      SOURCE_QUEUE_URL: "${SQS_ENDPOINT}/queue/${DEPLOYMENT_NAME}-node-identifier-queue"
      <<: *sqs-env
      <<: *rust-backtrace
    tty: false
    depends_on:
      provisioner:
        condition: service_completed_successfully
      localstack:
        condition: service_healthy
      redis:
        condition: service_healthy

  grapl-node-identifier-retry-handler:
    image: grapl/grapl-node-identifier-retry-handler:${TAG:-latest}
    environment:
      <<: *aws-region
      DEAD_LETTER_QUEUE_URL: "${SQS_ENDPOINT}/queue/${DEPLOYMENT_NAME}-node-identifier-dead-letter-queue"
      DEST_BUCKET_NAME: "${DEPLOYMENT_NAME}-subgraphs-generated-bucket"
      <<: *dynamodb-env
      <<: *dynamodb-mapping-tables
      REDIS_ENDPOINT: "${REDIS_ENDPOINT}"
      <<: *log-level
      RETRY_QUEUE_URL: "${SQS_ENDPOINT}/queue/${DEPLOYMENT_NAME}-node-identifier-dead-letter-queue"
      <<: *s3-env
      SOURCE_QUEUE_URL: "${SQS_ENDPOINT}/queue/${DEPLOYMENT_NAME}-node-identifier-retry-queue"
      <<: *sqs-env
      <<: *rust-backtrace
    tty: false
    depends_on:
      provisioner:
        condition: service_completed_successfully
      localstack:
        condition: service_healthy
      redis:
        condition: service_healthy

  grapl-graph-merger:
    image: grapl/grapl-graph-merger:${TAG:-latest}
    environment:
      <<: *aws-region
      DEAD_LETTER_QUEUE_URL: "${SQS_ENDPOINT}/queue/${DEPLOYMENT_NAME}-graph-merger-dead-letter-queue"
      DEST_BUCKET_NAME: "${DEPLOYMENT_NAME}-subgraphs-merged-bucket"
      <<: *dgraph-env
      <<: *dynamodb-env
      REDIS_ENDPOINT: "${REDIS_ENDPOINT}"
      GRAPL_SCHEMA_TABLE: "${DEPLOYMENT_NAME}-grapl_schema_table"
      <<: *log-level
      RETRY_QUEUE_URL: "${SQS_ENDPOINT}/queue/${DEPLOYMENT_NAME}-graph-merger-retry-queue"
      <<: *s3-env
      SOURCE_QUEUE_URL: "${SQS_ENDPOINT}/queue/${DEPLOYMENT_NAME}-graph-merger-queue"
      <<: *sqs-env
      <<: *rust-backtrace
    tty: false
    depends_on:
      dgraph:
        condition: service_started
      provisioner:
        condition: service_completed_successfully
      localstack:
        condition: service_healthy
      redis:
        condition: service_healthy

  grapl-analyzer-dispatcher:
    image: grapl/grapl-analyzer-dispatcher:${TAG:-latest}
    environment:
      ANALYZER_BUCKET: "${DEPLOYMENT_NAME}-analyzers-bucket"
      <<: *aws-region
      DEAD_LETTER_QUEUE_URL: "${SQS_ENDPOINT}/queue/${DEPLOYMENT_NAME}-analyzer-dispatcher-dead-letter-queue"
      DEST_BUCKET_NAME: "${DEPLOYMENT_NAME}-dispatched-analyzer-bucket"
      <<: *log-level
      RETRY_QUEUE_URL: "${SQS_ENDPOINT}/queue/${DEPLOYMENT_NAME}-analyzer-dispatcher-retry-queue"
      <<: *s3-env
      SOURCE_QUEUE_URL: "${SQS_ENDPOINT}/queue/${DEPLOYMENT_NAME}-analyzer-dispatcher-queue"
      <<: *sqs-env
      <<: *rust-backtrace
    tty: false
    depends_on:
      localstack:
        condition: service_healthy

  ########################################################################
  # Python Services
  ########################################################################

  grapl-analyzer-executor:
    image: grapl/grapl-analyzer-executor:${TAG:-latest}
    environment:
      <<: *aws-region
      DEPLOYMENT_NAME: ${DEPLOYMENT_NAME}
      DEBUG_SERVICES: "${DEBUG_SERVICES:-}"
      <<: *dgraph-env
      GRPC_ENABLE_FORK_SUPPORT: "1"
      HITCACHE_ADDR: "${REDIS_HOST}"
      HITCACHE_PORT: "${REDIS_PORT}"
      IS_RETRY: "False"
      <<: *log-level
      MESSAGECACHE_ADDR: "${REDIS_HOST}"
      MESSAGECACHE_PORT: "${REDIS_PORT}"
      RETRY_QUEUE_URL: "${SQS_ENDPOINT}/queue/${DEPLOYMENT_NAME}-analyzer-executor-retry-queue"
      <<: *s3-env
      SOURCE_QUEUE_URL: "${SQS_ENDPOINT}/queue/${DEPLOYMENT_NAME}-analyzer-executor-queue"
      <<: *sqs-env
      VSC_DEBUGGER_PORT: "${VSC_DEBUGGER_PORT_FOR_ANALYZER_EXECUTOR}"
    tty: true
    ports:
      - 127.0.0.1:${VSC_DEBUGGER_PORT_FOR_ANALYZER_EXECUTOR}:${VSC_DEBUGGER_PORT_FOR_ANALYZER_EXECUTOR}
    depends_on:
      dgraph:
        condition: service_started
      provisioner:
        condition: service_completed_successfully
      localstack:
        condition: service_healthy
      redis:
        condition: service_healthy

  grapl-model-plugin-deployer:
    image: grapl/grapl-model-plugin-deployer:${TAG:-latest}
    command: |
      /bin/sh -c '
        . venv/bin/activate &&
        cd /home/grapl/app &&
        chalice local \
          --no-autoreload \
          --host=0.0.0.0 \
          --port=${GRAPL_MODEL_PLUGIN_DEPLOYER_PORT}
      '
    environment:
      <<: *aws-region
      <<: *dgraph-env
      <<: *dynamodb-env
      IS_LOCAL: "True"
      <<: *log-level
      <<: *s3-env
      <<: *secretsmanager-env
      UX_BUCKET_URL: "localhost"
    tty: true
    depends_on:
      dgraph:
        condition: service_started
      localstack:
        condition: service_healthy
      provisioner:
        condition: service_completed_successfully
    ports:
      - 127.0.0.1:${GRAPL_MODEL_PLUGIN_DEPLOYER_PORT}:${GRAPL_MODEL_PLUGIN_DEPLOYER_PORT}
    networks:
      default:
        aliases:
          - ${GRAPL_MODEL_PLUGIN_DEPLOYER_HOST}

  ########################################################################
  # Web Services
  ########################################################################

  nginx:
    image: nginxinc/nginx-unprivileged
    command: |
      /bin/bash -c "
        export API_GATEWAY_API_ID=$$(cat /pulumi-outputs/prod-api-id) &&
        /docker-entrypoint.sh nginx -g 'daemon off;'
      "
    volumes:
      - ./etc/local_grapl/nginx_templates:/etc/nginx/templates
      - type: volume
        source: pulumi_outputs
        target: /pulumi-outputs
        read_only: true
    ports:
      - "127.0.0.1:1234:${GRAPL_HTTP_FRONTEND_PORT}"
    environment:
      - GRAPL_GRAPHQL_HOST
      - GRAPL_GRAPHQL_PORT
      - GRAPL_MODEL_PLUGIN_DEPLOYER_HOST
      - GRAPL_MODEL_PLUGIN_DEPLOYER_PORT
      - LOCALSTACK_HOST
      - LOCALSTACK_PORT
    depends_on:
      grapl-model-plugin-deployer:
        condition: service_started
      grapl-graphql-endpoint:
        condition: service_started
      grapl-pulumi:
        # We must wait until Pulumi has created the API gateway so we
        # know what its URL is.
        condition: service_completed_successfully
    networks:
      default:
        aliases:
          - ${GRAPL_API_HOST}

  grapl-engagement-view-uploader:
    image: grapl/grapl-engagement-view:${TAG:-latest}
    command: |
      /bin/bash -c "./upload_local.sh"
    environment:
      <<: *aws-region
      <<: *s3-env
    depends_on:
      provisioner:
        condition: service_completed_successfully

  grapl-graphql-endpoint:
    image: grapl/grapl-graphql-endpoint:${TAG:-latest}
    command: bash -c "./start_potentially_with_debugger.sh"
    environment:
      <<: *aws-region
      <<: *dgraph-env
      <<: *dynamodb-env
      IS_LOCAL: "True"
      JWT_SECRET_ID: "JWT_SECRET_ID"
      PORT: ${GRAPL_GRAPHQL_PORT}
      GRAPL_SCHEMA_PROPERTIES_TABLE: "${DEPLOYMENT_NAME}-grapl_schema_properties_table"
      GRAPL_SCHEMA_TABLE: "${DEPLOYMENT_NAME}-grapl_schema_table"
      <<: *dynamodb-env
      <<: *secretsmanager-env
      DEBUG_SERVICES: "${DEBUG_SERVICES:-}"
      VSC_DEBUGGER_PORT_FOR_GRAPHQL_ENDPOINT: "${VSC_DEBUGGER_PORT_FOR_GRAPHQL_ENDPOINT}"
    depends_on:
      dgraph:
        condition: service_started
      localstack:
        condition: service_healthy
      provisioner:
        condition: service_completed_successfully
    ports:
      - 127.0.0.1:${GRAPL_GRAPHQL_PORT}:${GRAPL_GRAPHQL_PORT}
      - 127.0.0.1:${VSC_DEBUGGER_PORT_FOR_GRAPHQL_ENDPOINT}:${VSC_DEBUGGER_PORT_FOR_GRAPHQL_ENDPOINT}
    networks:
      default:
        aliases:
          - ${GRAPL_GRAPHQL_HOST}

  grapl-notebook:
    image: grapl/grapl-notebook:${TAG:-latest}
    user: grapl
    environment:
      <<: *dgraph-env
    depends_on:
      - dgraph
    ports:
      - 127.0.0.1:${GRAPL_NOTEBOOK_PORT}:${GRAPL_NOTEBOOK_PORT}

  ########################################################################
  # Utility Services
  ########################################################################

  grapl-pulumi:
    image: grapl/grapl-local-pulumi:${TAG:-latest}
    command: |
      /bin/bash -c "
         pulumi login --local &&
         pulumi stack init local-grapl --non-interactive &&
         pulumi up --yes --skip-preview --stack=local-grapl &&

         # Write the necessary outputs to the shared volume, for access by other containers
         pulumi stack output prod-api-id > /home/grapl/pulumi-outputs/prod-api-id
      "
    # Our local-grapl Pulumi stack is configured to communicate with
    # localhost. By participating in the network namespace of our
    # localstack container, we can use the same stack configuration
    # both inside this compose network, as well as from outside on our
    # workstations.
    network_mode: "service:localstack"
    volumes:
      - type: bind
        source: ./src/aws-provision/zips
        target: /home/grapl/src/aws-provision/zips
        read_only: true
      - type: volume
        source: pulumi_outputs
        target: /home/grapl/pulumi-outputs
        read_only: false
    environment:
      PULUMI_CONFIG_PASSPHRASE: local-grapl-passphrase
      # Other environment variables like MG_ALPHAS are passed in via
      # Pulumi.local-grapl.yaml
    depends_on:
      localstack:
        condition: service_healthy

  provisioner:
    image: grapl/provisioner:${TAG:-latest}
    command: |
      /bin/bash -c "
          python -c 'import lambdex_handler; lambdex_handler.handler(None, None)'
      "
    environment:
      <<: *aws-region
      <<: *dgraph-env
      <<: *dynamodb-env
      <<: *secretsmanager-env
      <<: *sqs-env
      <<: *sns-env
      <<: *ec2-env
      <<: *ssm-env
      <<: *cloudwatch-env
      <<: *lambda-env
      <<: *route53-env
      <<: *s3-env
      GRAPL_LOG_LEVEL: "${GRAPL_LOG_LEVEL:-INFO}"
      AWS_ACCESS_KEY_ID: "${FAKE_AWS_ACCESS_KEY_ID}"
      AWS_SECRET_ACCESS_KEY: "${FAKE_AWS_SECRET_ACCESS_KEY}"
      DEPLOYMENT_NAME: "${DEPLOYMENT_NAME}"
      GRAPL_TEST_USER_NAME: "${DEPLOYMENT_NAME}-grapl-test-user"
    tty: true
    depends_on:
      dgraph:
        condition: service_started
      localstack:
        condition: service_healthy
      grapl-pulumi:
        condition: service_completed_successfully

  kafka-provision:
    image: confluentinc/cp-kafka:6.2.0
    command: |
      /bin/bash -c "! kafka-topics \
                    --create \
                    --zookeeper \
                    ${ZOOKEEPER_HOST}:${ZOOKEEPER_PORT} \
                    --replication-factor 1 \
                    --partitions 1 \
                    --topic test-topic \
                    --if-not-exists
      "
    restart: on-failure
    depends_on:
      kafka-broker:
        condition: service_healthy

networks:
  default:
    name: grapl-network
